{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader, PDFMinerLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain.embeddings import SentenceTransformerEmbeddings \n",
    "from langchain.vectorstores import Chroma \n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA \n",
    "from constants import CHROMA_SETTINGS\n",
    "#from streamlit_chat import message\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM \n",
    "from transformers import pipeline\n",
    "\n",
    "checkpoint = \"models/LaMini-T5-738M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_pipeline():\n",
    "    pipe = pipeline(\n",
    "        'text2text-generation',\n",
    "        model = base_model,\n",
    "        tokenizer = tokenizer,\n",
    "        max_length = 256,\n",
    "        do_sample = True,\n",
    "        temperature = 0.3,\n",
    "        top_p= 0.95\n",
    "        \n",
    "    )\n",
    "    local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    return local_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_llm():\n",
    "    llm = llm_pipeline()\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"models/all-MiniLM-L6-v2\")\n",
    "    db = Chroma(persist_directory=\"db\", embedding_function = embeddings, collection_name='VectorDB')\n",
    "    retriever = db.as_retriever()\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm = llm,\n",
    "        chain_type = \"stuff\",\n",
    "        retriever = retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_answer(instruction):\n",
    "    response = ''\n",
    "    instruction = instruction\n",
    "    qa = qa_llm()\n",
    "    generated_text = qa(instruction)\n",
    "    answer = generated_text['result']\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The minimum storypoint for Bumblebee Pod is 1.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_answer(\"what is the minimum storypoint for bumblebee pod\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
